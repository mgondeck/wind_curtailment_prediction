{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVryB0MPzOljoR+vuQjrFm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**AUTOCORRELATION PLOTS AND HOW TO INTERPRET THEM**\n",
        "\n",
        "Interpreting and reading an autocorrelation plot:\n",
        "\n",
        "The autocorrelation plot shows the correlation of the time series data with itself at different lags.\n",
        "The x-axis represents the lag, which is the time shift.\n",
        "The y-axis represents the autocorrelation coefficient, which measures the strength and direction of the correlation.\n",
        "The shaded region represents the confidence intervals, typically at 95% confidence level. Points outside this region are considered statistically significant.\n",
        "A positive autocorrelation indicates a positive relationship between the current observation and the observation at the lagged time point.\n",
        "A negative autocorrelation indicates a negative relationship between the current observation and the observation at the lagged time point.\n",
        "Autocorrelation values closer to 1 or -1 indicate a stronger correlation, while values closer to 0 indicate weaker correlation."
      ],
      "metadata": {
        "id": "lDSL9mx8shWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WHY NOT TO USE TRAIN TEST SPLIT WITH TIME SERIES**\n",
        "\n",
        "Here's a simple explanation of why it's not a good idea to shuffle data when working with time series:\n",
        "\n",
        "Imagine that you have a bunch of data points that represent things that happened over time. Each data point has a time stamp, like a date, that tells you when it happened.\n",
        "\n",
        "Now, imagine that you mix up all of the data points and put them in a different order. This is like shuffling a deck of cards. It might seem like a good idea because it helps you check if your model is working well, but it's actually not a good idea because the order of the data points is important.\n",
        "\n",
        "The data points are like a story, and the order they happened in is important to understanding the story. If you shuffle the data points, it's like telling the story out of order, and it doesn't make sense anymore. It's hard for your model to learn from the data if the data doesn't make sense.\n",
        "\n",
        "So, when you're working with time series data, it's important to keep the data points in the order that they happened. This way, your model can learn from the data and make good predictions."
      ],
      "metadata": {
        "id": "7Jy-j7pc-lts"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWNvhPN9sg6T"
      },
      "outputs": [],
      "source": [
        "# load packages\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, precision_score, f1_score, recall_score\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDP6aCtwBrUd",
        "outputId": "12d87597-6efd-48c1-dfdd-ddaa4d23c4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load lagged dataset\n",
        "df = pd.read_csv('/content/drive/My Drive/ms_wind_curtailment_prediction/lagged_curtailment_target_features.csv', sep = ';', index_col=0)"
      ],
      "metadata": {
        "id": "tGadVrdoBtK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting index from object type to datetime type\n",
        "df.index = pd.to_datetime(df.index)"
      ],
      "metadata": {
        "id": "YPu7Opc7Ch8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKXOyUOfCr-a",
        "outputId": "aa3efac9-de89-4851-be01-de045a89392f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['redispatch', 'level', 'wind_speed_m/s', 'wind_speed_m/s_lag1',\n",
              "       'wind_speed_m/s_lag2', 'wind_speed_m/s_lag3', 'wind_speed_m/s_lag4',\n",
              "       'wind_speed_m/s_lag5', 'wind_direction_degrees',\n",
              "       'wind_direction_degrees_lag1', 'wind_direction_degrees_lag2',\n",
              "       'wind_direction_degrees_lag3', 'wind_direction_degrees_lag4',\n",
              "       'wind_direction_degrees_lag5', 'radiation_global_J/m2',\n",
              "       'radiation_global_J/m2_lag1', 'radiation_global_J/m2_lag2',\n",
              "       'radiation_global_J/m2_lag3', 'radiation_global_J/m2_lag4',\n",
              "       'radiation_global_J/m2_lag5', 'air_temperature_K',\n",
              "       'air_temperature_K_lag1', 'air_temperature_K_lag2',\n",
              "       'air_temperature_K_lag3', 'air_temperature_K_lag4',\n",
              "       'air_temperature_K_lag5', 'humidity_percent', 'humidity_percent_lag1',\n",
              "       'humidity_percent_lag2', 'humidity_percent_lag3',\n",
              "       'humidity_percent_lag4', 'humidity_percent_lag5', 'wind_gust_max_m/s',\n",
              "       'wind_gust_max_m/s_lag1', 'wind_gust_max_m/s_lag2',\n",
              "       'wind_gust_max_m/s_lag3', 'wind_gust_max_m/s_lag4',\n",
              "       'wind_gust_max_m/s_lag5', 'wind_direction_gust_max_degrees',\n",
              "       'wind_direction_gust_max_degrees_lag1',\n",
              "       'wind_direction_gust_max_degrees_lag2',\n",
              "       'wind_direction_gust_max_degrees_lag3',\n",
              "       'wind_direction_gust_max_degrees_lag4',\n",
              "       'wind_direction_gust_max_degrees_lag5', 'forecast_solar_MW',\n",
              "       'forecast_solar_MW_lag1', 'forecast_solar_MW_lag2',\n",
              "       'forecast_solar_MW_lag3', 'forecast_solar_MW_lag4',\n",
              "       'forecast_solar_MW_lag5', 'total_grid_load_MWh',\n",
              "       'total_grid_load_MWh_lag1', 'total_grid_load_MWh_lag2',\n",
              "       'total_grid_load_MWh_lag3', 'total_grid_load_MWh_lag4',\n",
              "       'total_grid_load_MWh_lag5', 'residual_load_MWh',\n",
              "       'residual_load_MWh_lag1', 'residual_load_MWh_lag2',\n",
              "       'residual_load_MWh_lag3', 'residual_load_MWh_lag4',\n",
              "       'residual_load_MWh_lag5', 'pumped_storage_MWh',\n",
              "       'pumped_storage_MWh_lag1', 'pumped_storage_MWh_lag2',\n",
              "       'pumped_storage_MWh_lag3', 'pumped_storage_MWh_lag4',\n",
              "       'pumped_storage_MWh_lag5'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify significant lag values (where autocorrelation is outside the confidence intervals)\n",
        "significant_lags = [48, 96]\n",
        "\n",
        "df_lagged = df.copy()\n",
        "\n",
        "# Create lagged features\n",
        "for lag in significant_lags:\n",
        "    df_lagged[f'redispatch_lag_{lag}'] = df_lagged['redispatch'].shift(lag)\n",
        "    df_lagged[f'level_lag_{lag}'] = df_lagged['level'].shift(lag)\n",
        "\n",
        "# Drop rows with NaN values resulting from the shifting\n",
        "df_lagged.dropna(inplace=True)\n"
      ],
      "metadata": {
        "id": "ipQe1QPnDGoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get desired df size\n",
        "start_date = '2022-01-01'\n",
        "end_date = '2023-06-30'\n",
        "df_lagged = df_lagged.loc[start_date:end_date]"
      ],
      "metadata": {
        "id": "TTaRk2MCI94P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING XGBOOST WITHOUT SMOTE\n",
        "\n",
        "but handling class imbalance with scale_pos_weight = ratio\n",
        "\n",
        "Average Train F1 Score: 0.5314998790499398\n",
        "Average Train Precision: 0.5898975814577239\n",
        "Average Test F1 Score: 0.16610137820824686\n",
        "Average Test Precision: 0.22274850608643465\n",
        "\n",
        "\n",
        "2. ratio = 5\n",
        "\n",
        "Average Train F1 Score: 0.47032209963494065\n",
        "Average Train Precision: 0.6848833081255093\n",
        "Average Test F1 Score: 0.13780767453150464\n",
        "Average Test Precision: 0.25665706143264116\n",
        "\n",
        "3. ratio = 4\n",
        "\n",
        "Average Train F1 Score: 0.4364925588864709\n",
        "Average Train Precision: 0.7113404944095644\n",
        "Average Test F1 Score: 0.1262201619767872\n",
        "Average Test Precision: 0.26329427123769217\n",
        "\n",
        "4. ratio =6; threshold = 0.45\n",
        "\n",
        "Average Train F1 Score: 0.49843291517629684\n",
        "Average Train Precision: 0.653480149643088\n",
        "Average Train Recall: 0.44467934155177663\n",
        "\n",
        " Average Test F1 Score: 0.1413042411667109\n",
        "Average Test Precision: 0.25175147014687826\n",
        "Average Test Recall: 0.12706465765116165\n",
        "\n",
        " Average Threshold F1 Score: 0.1659465367087471\n",
        "Average Threshold Precision: 0.25264179317835117\n",
        "Average Threshold Recall: 0.16769221937182072\n",
        "\n",
        "5. ratio = 10; threshold = 0.6; n_splits = 10\n",
        "\n",
        "Average Train F1 Score: 0.5673634598155803\n",
        "Average Train Precision: 0.6788178041013995\n",
        "Average Train Recall: 0.5649233506536049\n",
        "\n",
        " Average Test F1 Score: 0.15600429057526033\n",
        "Average Test Precision: 0.32111401008649665\n",
        "Average Test Recall: 0.1792308401831178\n",
        "\n",
        " Average Threshold F1 Score: 0.12516613010671776\n",
        "Average Threshold Precision: 0.4552949064158541\n",
        "Average Threshold Recall: 0.12271878954130425\n",
        "\n",
        "6. same as above and for all years\n",
        "\n",
        "Average Train F1 Score: 0.6271271587571896\n",
        "Average Train Precision: 0.719607310683743\n",
        "Average Train Recall: 0.6314889332056188\n",
        "\n",
        " Average Test F1 Score: 0.15124855685122585\n",
        "Average Test Precision: 0.41402793661481174\n",
        "Average Test Recall: 0.18656238738548198\n",
        "\n",
        " Average Threshold F1 Score: 0.12338705210525355\n",
        "Average Threshold Precision: 0.5715504495040475\n",
        "Average Threshold Recall: 0.1292740934462511"
      ],
      "metadata": {
        "id": "5yT3dxiEtfG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "features = df_lagged.columns\n",
        "\n",
        "target = 'redispatch'\n",
        "\n",
        "X = df_lagged[features].drop(['redispatch', 'level'], axis=1)\n",
        "y = df_lagged[target]\n",
        "\n",
        "# Define hyperparameters\n",
        "params = {\n",
        "    'max_depth': 7,\n",
        "    'min_child_weight': 10,\n",
        "    'gamma': 0.2,\n",
        "    'subsample': 0.5,\n",
        "    'colsample_bytree': 0.5,\n",
        "    'booster': 'gbtree',\n",
        "    'reg_alpha': 4,\n",
        "    'reg_lambda': 4,\n",
        "    'n_estimators': 100,\n",
        "    'learning_rate': 0.1,\n",
        "    'objective': 'binary:logistic',\n",
        "    'random_state': 42,\n",
        "    'verbosity': 0,\n",
        "    #'early_stopping_rounds': 20,\n",
        "    'scale_pos_weight': 1\n",
        "}\n",
        "\n",
        "# Define the number of splits for time series cross-validation\n",
        "n_splits = 10  # You can adjust this value as needed\n",
        "gap = 48  # 12 hour difference between train and test sets\n",
        "\n",
        "# Initialize lists to store evaluation metrics for each fold\n",
        "train_f1_scores = []\n",
        "train_precision_scores = []\n",
        "train_recall_scores = []\n",
        "test_f1_scores = []\n",
        "test_precision_scores = []\n",
        "test_recall_scores = []\n",
        "thresh_f1_scores = []\n",
        "thresh_precision_scores = []\n",
        "thresh_recall_scores = []\n",
        "\n",
        "# Define the time series splitter\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits, gap=gap)\n",
        "\n",
        "# Define threshold for precision calculation\n",
        "threshold = 0.7\n",
        "\n",
        "# Instantiate SMOTE\n",
        "smote = SMOTE(k_neighbors=1, random_state=42)\n",
        "\n",
        "# Iterate over each fold\n",
        "for fold, (train_index, test_index) in enumerate(tscv.split(X), 1):\n",
        "    print(f\"Training on fold {fold}/{n_splits}\")\n",
        "    # Get the data for this fold\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Apply SMOTE to balance the classes\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Initialize and train the XGBoost classifier with early stopping\n",
        "    model = XGBClassifier(**params)\n",
        "\n",
        "    #eval_set = [(X_train_resampled, y_train_resampled), (X_test, y_test)]  # Evaluation sets for early stopping\n",
        "    model.fit(X_train_resampled, y_train_resampled, verbose=0)#, eval_set=eval_set)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model using precision, recall, and F1 score\n",
        "    train_f1 = f1_score(y_train, y_train_pred, zero_division=1)\n",
        "    train_precision = precision_score(y_train, y_train_pred, zero_division=1)\n",
        "    train_recall = recall_score(y_train, y_train_pred, zero_division=1)\n",
        "    test_f1 = f1_score(y_test, y_test_pred, zero_division=1)\n",
        "    test_precision = precision_score(y_test, y_test_pred, zero_division=1)\n",
        "    test_recall = recall_score(y_test, y_test_pred, zero_division=1)\n",
        "\n",
        "    # Compute precision, recall, and F1 score using the specified threshold\n",
        "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
        "    y_test_pred_threshold = (y_test_proba > threshold).astype(int)\n",
        "    fold_precision = precision_score(y_test, y_test_pred_threshold, zero_division=1)\n",
        "    fold_recall = recall_score(y_test, y_test_pred_threshold, zero_division=1)\n",
        "    fold_f1 = f1_score(y_test, y_test_pred_threshold, zero_division=1)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_test_pred_threshold)\n",
        "    # Print the confusion matrix\n",
        "    print(f\"Confusion Matrix (Fold {fold}):\")\n",
        "    print(cm)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Store the evaluation metrics for this fold\n",
        "    train_f1_scores.append(train_f1)\n",
        "    train_precision_scores.append(train_precision)\n",
        "    train_recall_scores.append(train_recall)\n",
        "    test_f1_scores.append(test_f1)\n",
        "    test_precision_scores.append(test_precision)\n",
        "    test_recall_scores.append(test_recall)\n",
        "    thresh_f1_scores.append(fold_f1)\n",
        "    thresh_precision_scores.append(fold_precision)\n",
        "    thresh_recall_scores.append(fold_recall)\n",
        "\n",
        "# Calculate average evaluation metrics across all folds\n",
        "avg_train_f1 = np.mean(train_f1_scores)\n",
        "avg_train_precision = np.mean(train_precision_scores)\n",
        "avg_train_recall = np.mean(train_recall_scores)\n",
        "avg_test_f1 = np.mean(test_f1_scores)\n",
        "avg_test_precision = np.mean(test_precision_scores)\n",
        "avg_test_recall = np.mean(test_recall_scores)\n",
        "avg_thresh_f1_scores = np.mean(thresh_f1_scores)\n",
        "avg_thresh_precision_scores = np.mean(thresh_precision_scores)\n",
        "avg_thresh_recall_scores = np.mean(thresh_recall_scores)\n",
        "\n",
        "# Print the results\n",
        "print(\"Average Train F1 Score:\", avg_train_f1)\n",
        "print(\"Average Train Precision:\", avg_train_precision)\n",
        "print(\"Average Train Recall:\", avg_train_recall)\n",
        "print(\"\\n Average Test F1 Score:\", avg_test_f1)\n",
        "print(\"Average Test Precision:\", avg_test_precision)\n",
        "print(\"Average Test Recall:\", avg_test_recall)\n",
        "print(\"\\n Average Threshold F1 Score:\", avg_thresh_f1_scores)\n",
        "print(\"Average Threshold Precision:\", avg_thresh_precision_scores)\n",
        "print(\"Average Threshold Recall:\", avg_thresh_recall_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKoGQmPttG0u",
        "outputId": "696769f4-d298-4ad3-f3b3-dff9118d5641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on fold 1/10\n",
            "Confusion Matrix (Fold 1):\n",
            "[[4643    0]\n",
            " [ 123    0]]\n",
            "\n",
            "\n",
            "Training on fold 2/10\n",
            "Confusion Matrix (Fold 2):\n",
            "[[4211    7]\n",
            " [ 548    0]]\n",
            "\n",
            "\n",
            "Training on fold 3/10\n",
            "Confusion Matrix (Fold 3):\n",
            "[[3591  173]\n",
            " [ 598  404]]\n",
            "\n",
            "\n",
            "Training on fold 4/10\n",
            "Confusion Matrix (Fold 4):\n",
            "[[3631  205]\n",
            " [ 728  202]]\n",
            "\n",
            "\n",
            "Training on fold 5/10\n",
            "Confusion Matrix (Fold 5):\n",
            "[[3064  469]\n",
            " [ 841  392]]\n",
            "\n",
            "\n",
            "Training on fold 6/10\n",
            "Confusion Matrix (Fold 6):\n",
            "[[2765  335]\n",
            " [1204  462]]\n",
            "\n",
            "\n",
            "Training on fold 7/10\n",
            "Confusion Matrix (Fold 7):\n",
            "[[3985  195]\n",
            " [ 548   38]]\n",
            "\n",
            "\n",
            "Training on fold 8/10\n",
            "Confusion Matrix (Fold 8):\n",
            "[[4095  128]\n",
            " [ 472   71]]\n",
            "\n",
            "\n",
            "Training on fold 9/10\n",
            "Confusion Matrix (Fold 9):\n",
            "[[4200  165]\n",
            " [ 232  169]]\n",
            "\n",
            "\n",
            "Training on fold 10/10\n",
            "Confusion Matrix (Fold 10):\n",
            "[[4405  225]\n",
            " [  67   69]]\n",
            "\n",
            "\n",
            "Average Train F1 Score: 0.7901081619647353\n",
            "Average Train Precision: 0.7418472727699695\n",
            "Average Train Recall: 0.8457980695236111\n",
            "\n",
            " Average Test F1 Score: 0.3035361744208359\n",
            "Average Test Precision: 0.3232018962743025\n",
            "Average Test Recall: 0.3239491246508429\n",
            "\n",
            " Average Threshold F1 Score: 0.26284055549837515\n",
            "Average Threshold Precision: 0.4492002085904283\n",
            "Average Threshold Recall: 0.2340033407454051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# safe XGBoost classifier\n",
        "joblib.dump(model, '/content/drive/My Drive/ms_wind_curtailment_prediction/xgboost_class.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMT6CwGQ-jn5",
        "outputId": "44be3cb7-e816-459e-828d-2fdd943994dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/ms_wind_curtailment_prediction/xgboost_class.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# impute missing values for forecast solar with actual solar\n",
        "for index, row in df.iterrows():\n",
        "    if pd.isna(row['forecast_solar_MW']):\n",
        "        df.at[index, 'forecast_solar_MW'] = row['actual_solar_MW']\n",
        "\n",
        "# impute other missing values by interpolation\n",
        "columns_to_interpolate = [\"wind_speed_m/s\",  \"wind_direction_degrees\", \"humidity_percent\", \"radiation_global_J/m2\", \"air_temperature_K\", \"wind_gust_max_m/s\", \"wind_direction_gust_max_degrees\", \"forecast_solar_MW\", \"total_grid_load_MWh\", \"residual_load_MWh\", \"pumped_storage_MWh\"]\n",
        "df[columns_to_interpolate] = df[columns_to_interpolate].interpolate(method='linear', limit_direction='both')\n",
        "df.drop(\"actual_solar_MW\", axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "j-RS1VJxB1Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Assuming df is your DataFrame with a datetime index\n",
        "# Create a new column for weekday/weekend\n",
        "df['weekday'] = df.index.weekday < 5\n",
        "\n",
        "# Create a new column for season based on month\n",
        "df['season'] = (df.index.month % 12 + 3) // 3\n",
        "\n",
        "# Perform one-hot encoding for the 'season' column\n",
        "df = pd.get_dummies(df, columns=['season'], drop_first=True)\n",
        "\n",
        "# Perform one-hot encoding for the 'weekday' column\n",
        "df = pd.get_dummies(df, columns=['weekday'], drop_first=True)\n",
        "\n",
        "# Convert the 'weekday' and 'season' columns to numerical\n",
        "df['weekday'] = df['weekday_True'].astype(int)\n",
        "df['season'] = df[['season_2', 'season_3', 'season_4']].idxmax(axis=1).str.extract(r'(\\d)').astype(int)\n",
        "\n",
        "# Drop the intermediate columns created during one-hot encoding\n",
        "df.drop(columns=['weekday_True', 'season_2', 'season_3', 'season_4'], inplace=True)\n",
        "\n",
        "# Now df contains the original data with the new weekday and season columns encoded numerically\n",
        "'''"
      ],
      "metadata": {
        "id": "9YPa10RhD_pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TESTING XGBOOST WITHOUT LAGGED FEATURES**\n",
        "\n",
        "1. no. of splits is 10\n",
        "2. train data is smoted\n",
        "3. level is dropped\n",
        "4. gap = 48, indicating a 12 hour time difference between train and test sets"
      ],
      "metadata": {
        "id": "CN42ruXgF6bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "features = ['wind_speed_m/s', 'wind_direction_degrees', 'radiation_global_J/m2',\n",
        "            'air_temperature_K', 'humidity_percent', 'wind_gust_max_m/s',\n",
        "            'wind_direction_gust_max_degrees', 'forecast_solar_MW',\n",
        "            'total_grid_load_MWh', 'residual_load_MWh', 'pumped_storage_MWh']\n",
        "target = 'redispatch'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Define the number of splits for time series cross-validation\n",
        "n_splits = 10  # You can adjust this value as needed\n",
        "gap = 48 #12 hour difference between train and test sets\n",
        "\n",
        "# Initialize lists to store evaluation metrics for each fold\n",
        "train_f1_scores = []\n",
        "train_precision_scores = []\n",
        "test_f1_scores = []\n",
        "test_precision_scores = []\n",
        "\n",
        "# Define the time series splitter\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits, gap=gap)\n",
        "\n",
        "# Iterate over each fold\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    # Get the data for this fold\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Handle class imbalance with SMOTE\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Initialize and train the XGBoost classifier\n",
        "    model = XGBClassifier()\n",
        "    model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train_resampled)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model using precision and F1 score\n",
        "    train_f1 = f1_score(y_train_resampled, y_train_pred)\n",
        "    train_precision = precision_score(y_train_resampled, y_train_pred)\n",
        "    test_f1 = f1_score(y_test, y_test_pred)\n",
        "    test_precision = precision_score(y_test, y_test_pred)\n",
        "\n",
        "    # Store the evaluation metrics for this fold\n",
        "    train_f1_scores.append(train_f1)\n",
        "    train_precision_scores.append(train_precision)\n",
        "    test_f1_scores.append(test_f1)\n",
        "    test_precision_scores.append(test_precision)\n",
        "\n",
        "# Calculate average evaluation metrics across all folds\n",
        "avg_train_f1 = np.mean(train_f1_scores)\n",
        "avg_train_precision = np.mean(train_precision_scores)\n",
        "avg_test_f1 = np.mean(test_f1_scores)\n",
        "avg_test_precision = np.mean(test_precision_scores)\n",
        "\n",
        "# Print the results\n",
        "print(\"Average Train F1 Score:\", avg_train_f1)\n",
        "print(\"Average Train Precision:\", avg_train_precision)\n",
        "print(\"Average Test F1 Score:\", avg_test_f1)\n",
        "print(\"Average Test Precision:\", avg_test_precision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfTxbz4iF5pu",
        "outputId": "bfc89419-9117-43e7-cdc8-35cd2dfb5be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Train F1 Score: 0.984944808880309\n",
            "Average Train Precision: 0.982622027331205\n",
            "Average Test F1 Score: 0.06338456555898733\n",
            "Average Test Precision: 0.08839974275175339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TESTING XGBOOST WITH LAGGED FEATURES**\n",
        "\n",
        "1. test 1 with 2 new cols\n",
        "  - added redispatch lag of 12h and level lag of 12h\n",
        "  - results:\n",
        "    - Average Train F1 Score: 0.9915501295944331\n",
        "    - Average Train Precision: 0.9920147267242493\n",
        "    - Average Test F1 Score: 0.10218923072315689\n",
        "    - Average Test Precision: 0.14244917998092851\n",
        "\n",
        "2. test 2 with 8 new cols\n",
        "  - added redispatch and level lag of 12h, 24h, 36h, and 48h\n",
        "  - results:\n",
        "    - Average Train F1 Score: 0.9948075541756023\n",
        "    - Average Train Precision: 0.9962690339292111\n",
        "    - Average Test F1 Score: 0.08688445428317475\n",
        "    - Average Test Precision: 0.13062294848350328\n",
        "\n",
        "3. test 3 with 4 new cols\n",
        "  - added redispatch and level lag of 12h, 24h\n",
        "  - results:\n",
        "    - Average Train F1 Score: 0.9936256724546532\n",
        "    - Average Train Precision: 0.9949062498419048\n",
        "    - Average Test F1 Score: 0.09647009249515685\n",
        "    - Average Test Precision: 0.1450875350687569\n",
        "\n",
        "4. test 4 with 4 new cols\n",
        "  - added redispatch and level lag of 12h and 24h\n",
        "  - XGBClassifier(max_depth=3, min_child_weight=5, gamma=0.1, subsample=0.8, colsample_bytree=0.8, eta=0.3, booster='gbtree', reg_alpha=4, reg_lambda=4, eval_metric='logloss', n_estimators=300, learning_rate=0.1, objective='binary:logistic', random_state = 42)\n",
        "  - results:\n",
        "    - Average Train F1 Score: 0.9688628383626243\n",
        "    - Average Train Precision: 0.9677430890306062\n",
        "    - Average Test F1 Score: 0.1571131836855964\n",
        "    - Average Test Precision: 0.16138542398417716\n",
        "\n",
        "5. test 5 with 4 new cols\n",
        "  - added redispatch and level lag of 12h and 24h\n",
        "  - XGBClassifier(max_depth=3, min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.8, eta=0.3, booster='gbtree', reg_alpha=4, reg_lambda=4, eval_metric='logloss', n_estimators=300, learning_rate=0.1, objective='binary:logistic', random_state = 42)\n",
        "  - Average Train F1 Score: 0.9693269618839041\n",
        "  - Average Train Precision: 0.9682704611850873\n",
        "  - Average Test F1 Score: 0.15884165476708328\n",
        "  - Average Test Precision: 0.1635677273311264\n",
        "\n",
        "6. test 6 with 4 new cols\n",
        "  - added redispatch and level lag of 12h and 24h\n",
        "  - XGBClassifier(max_depth=3, min_child_weight=1, gamma=0.1, subsample=0.6, colsample_bytree=0.8, eta=0.3, booster='gbtree', reg_alpha=4, reg_lambda=4, eval_metric='logloss', n_estimators=300, learning_rate=0.1, objective='binary:logistic', random_state = 42)\n",
        "  - Average Train F1 Score: 0.9692256171812323\n",
        "  - Average Train Precision: 0.9682865619100001\n",
        "  - Average Test F1 Score: 0.15976504869085842\n",
        "  - Average Test Precision: 0.16361172557789921\n",
        "\n",
        "7. test 7 with 4 new cols\n",
        "  - added redispatch and level lag of 12h and 24h\n",
        "  - XGBClassifier(max_depth=3, min_child_weight=1, gamma=0.1, subsample=0.6, colsample_bytree=0.8, eta=0.3, booster='gbtree', reg_alpha=4, reg_lambda=4, eval_metric='logloss', n_estimators=300, learning_rate=0.1, objective='binary:logistic', random_state = 42)\n",
        "\n",
        "Average Train F1 Score: 0.9691106998601267\n",
        "Average Train Precision: 0.9680982007749634\n",
        "Average Test F1 Score: 0.16098225252138793\n",
        "Average Test Precision: 0.16372665012292364\n",
        "\n",
        "8. Average Train F1 Score: 0.968794783239666\n",
        "Average Train Precision: 0.9674467229696168\n",
        "Average Test F1 Score: 0.1630161847366968\n",
        "Average Test Precision: 0.1644600450967673\n",
        "\n",
        "9. XGBClassifier(max_depth=3, min_child_weight=10, gamma=0.2, subsample=0.6, colsample_bytree=0.8, eta=0.3, booster='gbtree', reg_alpha=4, reg_lambda=4,\n",
        "                          n_estimators=300, learning_rate=0.1, objective='binary:logistic', random_state=42)\n",
        "    model.fit(X_train_resampled, y_train_resampled,\n",
        "              eval_set=[(X_train_resampled, y_train_resampled), (X_test, y_test)],\n",
        "              eval_metric='logloss',  Use the same eval_metric as specified in the model constructor\n",
        "              early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
        "\n",
        "Average Train F1 Score: 0.9384962863576518\n",
        "Average Train Precision: 0.9465448373438738\n",
        "Average Test F1 Score: 0.21852579635509956\n",
        "Average Test Precision: 0.18008337411938\n",
        "\n",
        "10. model = XGBClassifier(max_depth=3, min_child_weight=10, gamma=0.2, subsample=0.5, colsample_bytree=0.5, eta=0.5,\n",
        "                          booster='gbtree', reg_alpha=4, reg_lambda=4,\n",
        "                          n_estimators=300, learning_rate=0.1, objective='binary:logistic', random_state=42, verbosity=0)\n",
        "\n",
        "Average Train F1 Score: 0.9424078284610673\n",
        "Average Train Precision: 0.9481400026555029\n",
        "Average Test F1 Score: 0.22222587400809907\n",
        "Average Test Precision: 0.18362724806775338\n",
        "\n",
        "11. params = {\n",
        "    'max_depth': 3,\n",
        "    'min_child_weight': 10,\n",
        "    'gamma': 0.2,\n",
        "    'subsample': 0.5,\n",
        "    'colsample_bytree': 0.5,\n",
        "    'eta': 0.5,\n",
        "    'booster': 'gbtree',\n",
        "    'reg_alpha': 4,\n",
        "    'reg_lambda': 4,\n",
        "    'n_estimators': 500,\n",
        "    'learning_rate': 0.1,\n",
        "    'objective': 'binary:logistic',\n",
        "    'random_state': 42,\n",
        "    'verbosity': 0,\n",
        "    'early_stopping_rounds': 10  # Set early stopping rounds\n",
        "}\n",
        "\n",
        "Average Train F1 Score: 0.9446139030247693\n",
        "Average Train Precision: 0.950068119071387\n",
        "Average Test F1 Score: 0.22060981498923868\n",
        "Average Test Precision: 0.18575056299446638"
      ],
      "metadata": {
        "id": "QILIxEFhG4BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_lagged.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g-5VH_EH0_m",
        "outputId": "3deb9d9e-b6fd-41e0-e2a0-251a48ec768e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['redispatch', 'level', 'wind_speed_m/s', 'wind_speed_m/s_lag1',\n",
              "       'wind_speed_m/s_lag2', 'wind_speed_m/s_lag3', 'wind_speed_m/s_lag4',\n",
              "       'wind_speed_m/s_lag5', 'wind_direction_degrees',\n",
              "       'wind_direction_degrees_lag1', 'wind_direction_degrees_lag2',\n",
              "       'wind_direction_degrees_lag3', 'wind_direction_degrees_lag4',\n",
              "       'wind_direction_degrees_lag5', 'radiation_global_J/m2',\n",
              "       'radiation_global_J/m2_lag1', 'radiation_global_J/m2_lag2',\n",
              "       'radiation_global_J/m2_lag3', 'radiation_global_J/m2_lag4',\n",
              "       'radiation_global_J/m2_lag5', 'air_temperature_K',\n",
              "       'air_temperature_K_lag1', 'air_temperature_K_lag2',\n",
              "       'air_temperature_K_lag3', 'air_temperature_K_lag4',\n",
              "       'air_temperature_K_lag5', 'humidity_percent', 'humidity_percent_lag1',\n",
              "       'humidity_percent_lag2', 'humidity_percent_lag3',\n",
              "       'humidity_percent_lag4', 'humidity_percent_lag5', 'wind_gust_max_m/s',\n",
              "       'wind_gust_max_m/s_lag1', 'wind_gust_max_m/s_lag2',\n",
              "       'wind_gust_max_m/s_lag3', 'wind_gust_max_m/s_lag4',\n",
              "       'wind_gust_max_m/s_lag5', 'wind_direction_gust_max_degrees',\n",
              "       'wind_direction_gust_max_degrees_lag1',\n",
              "       'wind_direction_gust_max_degrees_lag2',\n",
              "       'wind_direction_gust_max_degrees_lag3',\n",
              "       'wind_direction_gust_max_degrees_lag4',\n",
              "       'wind_direction_gust_max_degrees_lag5', 'forecast_solar_MW',\n",
              "       'forecast_solar_MW_lag1', 'forecast_solar_MW_lag2',\n",
              "       'forecast_solar_MW_lag3', 'forecast_solar_MW_lag4',\n",
              "       'forecast_solar_MW_lag5', 'total_grid_load_MWh',\n",
              "       'total_grid_load_MWh_lag1', 'total_grid_load_MWh_lag2',\n",
              "       'total_grid_load_MWh_lag3', 'total_grid_load_MWh_lag4',\n",
              "       'total_grid_load_MWh_lag5', 'residual_load_MWh',\n",
              "       'residual_load_MWh_lag1', 'residual_load_MWh_lag2',\n",
              "       'residual_load_MWh_lag3', 'residual_load_MWh_lag4',\n",
              "       'residual_load_MWh_lag5', 'pumped_storage_MWh',\n",
              "       'pumped_storage_MWh_lag1', 'pumped_storage_MWh_lag2',\n",
              "       'pumped_storage_MWh_lag3', 'pumped_storage_MWh_lag4',\n",
              "       'pumped_storage_MWh_lag5', 'redispatch_lag_48', 'level_lag_48',\n",
              "       'redispatch_lag_96', 'level_lag_96'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "features = ['wind_speed_m/s', 'wind_speed_m/s_lag1',\n",
        "       'wind_direction_degrees', 'wind_direction_degrees_lag1',\n",
        "       'radiation_global_J/m2', 'radiation_global_J/m2_lag1',\n",
        "       'air_temperature_K', 'air_temperature_K_lag1', 'humidity_percent',\n",
        "       'humidity_percent_lag1', 'wind_gust_max_m/s', 'wind_gust_max_m/s_lag1',\n",
        "       'wind_direction_gust_max_degrees',\n",
        "       'wind_direction_gust_max_degrees_lag1', 'forecast_solar_MW',\n",
        "       'forecast_solar_MW_lag1', 'total_grid_load_MWh',\n",
        "       'total_grid_load_MWh_lag1', 'residual_load_MWh',\n",
        "       'residual_load_MWh_lag1', 'pumped_storage_MWh',\n",
        "       'pumped_storage_MWh_lag1', 'redispatch_lag_48', 'level_lag_48',\n",
        "       'redispatch_lag_96', 'level_lag_96']\n",
        "\n",
        "target = 'redispatch'\n",
        "\n",
        "X = df_lagged[features]\n",
        "y = df_lagged[target]\n",
        "\n",
        "# Define hyperparameters\n",
        "params = {\n",
        "    'max_depth': 3,\n",
        "    'min_child_weight': 10,\n",
        "    'gamma': 0.2,\n",
        "    'subsample': 0.5,\n",
        "    'colsample_bytree': 0.5,\n",
        "    'booster': 'gbtree',\n",
        "    'reg_alpha': 4,\n",
        "    'reg_lambda': 4,\n",
        "    'n_estimators': 500,\n",
        "    'learning_rate': 0.1,\n",
        "    'objective': 'binary:logistic',\n",
        "    'random_state': 42,\n",
        "    'verbosity': 0,\n",
        "    'early_stopping_rounds': 10,  # Set early stopping rounds\n",
        "    'scale_pos_weight': 1\n",
        "}\n",
        "\n",
        "# Define the number of splits for time series cross-validation\n",
        "n_splits = 10  # You can adjust this value as needed\n",
        "gap = 48  # 12 hour difference between train and test sets\n",
        "\n",
        "# Initialize lists to store evaluation metrics for each fold\n",
        "train_f1_scores = []\n",
        "train_precision_scores = []\n",
        "test_f1_scores = []\n",
        "test_precision_scores = []\n",
        "\n",
        "# Define the time series splitter\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits, gap=gap)\n",
        "\n",
        "# Iterate over each fold\n",
        "for fold, (train_index, test_index) in enumerate(tscv.split(X), 1):\n",
        "    print(f\"Training on fold {fold}/{n_splits}\")\n",
        "    # Get the data for this fold\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Handle class imbalance with SMOTE\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Initialize and train the XGBoost classifier with early stopping\n",
        "    model = XGBClassifier(**params)\n",
        "\n",
        "    eval_set = [(X_train_resampled, y_train_resampled), (X_test, y_test)]  # Evaluation sets for early stopping\n",
        "    model.fit(X_train_resampled, y_train_resampled, eval_set=eval_set, verbose=0)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train_resampled)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model using precision and F1 score\n",
        "    train_f1 = f1_score(y_train_resampled, y_train_pred)\n",
        "    train_precision = precision_score(y_train_resampled, y_train_pred)\n",
        "    test_f1 = f1_score(y_test, y_test_pred)\n",
        "    test_precision = precision_score(y_test, y_test_pred)\n",
        "\n",
        "    # Store the evaluation metrics for this fold\n",
        "    train_f1_scores.append(train_f1)\n",
        "    train_precision_scores.append(train_precision)\n",
        "    test_f1_scores.append(test_f1)\n",
        "    test_precision_scores.append(test_precision)\n",
        "\n",
        "# Calculate average evaluation metrics across all folds\n",
        "avg_train_f1 = np.mean(train_f1_scores)\n",
        "avg_train_precision = np.mean(train_precision_scores)\n",
        "avg_test_f1 = np.mean(test_f1_scores)\n",
        "avg_test_precision = np.mean(test_precision_scores)\n",
        "\n",
        "# Print the results\n",
        "print(\"Average Train F1 Score:\", avg_train_f1)\n",
        "print(\"Average Train Precision:\", avg_train_precision)\n",
        "print(\"Average Test F1 Score:\", avg_test_f1)\n",
        "print(\"Average Test Precision:\", avg_test_precision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92OQeCsbHvgy",
        "outputId": "c27093d1-e126-4112-cd6e-45efb0041333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on fold 1/10\n",
            "Training on fold 2/10\n",
            "Training on fold 3/10\n",
            "Training on fold 4/10\n",
            "Training on fold 5/10\n",
            "Training on fold 6/10\n",
            "Training on fold 7/10\n",
            "Training on fold 8/10\n",
            "Training on fold 9/10\n",
            "Training on fold 10/10\n",
            "Average Train F1 Score: 0.9375259803395558\n",
            "Average Train Precision: 0.9487513956232434\n",
            "Average Test F1 Score: 0.2117713012567693\n",
            "Average Test Precision: 0.17394025359851048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plj8DQIUtUTT",
        "outputId": "cd47f167-1927-42b6-e105-f1c6fced7e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    131714\n",
              "1.0      8398\n",
              "Name: redispatch, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_lagged.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVdzTl8H1lTS",
        "outputId": "6742ba20-dd92-4674-8d41-3168bc1dba53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['redispatch', 'level', 'wind_speed_m/s', 'wind_speed_m/s_lag1',\n",
              "       'wind_speed_m/s_lag2', 'wind_speed_m/s_lag3', 'wind_speed_m/s_lag4',\n",
              "       'wind_speed_m/s_lag5', 'wind_direction_degrees',\n",
              "       'wind_direction_degrees_lag1', 'wind_direction_degrees_lag2',\n",
              "       'wind_direction_degrees_lag3', 'wind_direction_degrees_lag4',\n",
              "       'wind_direction_degrees_lag5', 'radiation_global_J/m2',\n",
              "       'radiation_global_J/m2_lag1', 'radiation_global_J/m2_lag2',\n",
              "       'radiation_global_J/m2_lag3', 'radiation_global_J/m2_lag4',\n",
              "       'radiation_global_J/m2_lag5', 'air_temperature_K',\n",
              "       'air_temperature_K_lag1', 'air_temperature_K_lag2',\n",
              "       'air_temperature_K_lag3', 'air_temperature_K_lag4',\n",
              "       'air_temperature_K_lag5', 'humidity_percent', 'humidity_percent_lag1',\n",
              "       'humidity_percent_lag2', 'humidity_percent_lag3',\n",
              "       'humidity_percent_lag4', 'humidity_percent_lag5', 'wind_gust_max_m/s',\n",
              "       'wind_gust_max_m/s_lag1', 'wind_gust_max_m/s_lag2',\n",
              "       'wind_gust_max_m/s_lag3', 'wind_gust_max_m/s_lag4',\n",
              "       'wind_gust_max_m/s_lag5', 'wind_direction_gust_max_degrees',\n",
              "       'wind_direction_gust_max_degrees_lag1',\n",
              "       'wind_direction_gust_max_degrees_lag2',\n",
              "       'wind_direction_gust_max_degrees_lag3',\n",
              "       'wind_direction_gust_max_degrees_lag4',\n",
              "       'wind_direction_gust_max_degrees_lag5', 'forecast_solar_MW',\n",
              "       'forecast_solar_MW_lag1', 'forecast_solar_MW_lag2',\n",
              "       'forecast_solar_MW_lag3', 'forecast_solar_MW_lag4',\n",
              "       'forecast_solar_MW_lag5', 'total_grid_load_MWh',\n",
              "       'total_grid_load_MWh_lag1', 'total_grid_load_MWh_lag2',\n",
              "       'total_grid_load_MWh_lag3', 'total_grid_load_MWh_lag4',\n",
              "       'total_grid_load_MWh_lag5', 'residual_load_MWh',\n",
              "       'residual_load_MWh_lag1', 'residual_load_MWh_lag2',\n",
              "       'residual_load_MWh_lag3', 'residual_load_MWh_lag4',\n",
              "       'residual_load_MWh_lag5', 'pumped_storage_MWh',\n",
              "       'pumped_storage_MWh_lag1', 'pumped_storage_MWh_lag2',\n",
              "       'pumped_storage_MWh_lag3', 'pumped_storage_MWh_lag4',\n",
              "       'pumped_storage_MWh_lag5', 'redispatch_lag_48', 'level_lag_48',\n",
              "       'redispatch_lag_96', 'level_lag_96'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tqSMoubUJHW0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}