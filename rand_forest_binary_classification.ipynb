{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8RRfZoBnq9li6WH5xu83B"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iKiZHTHmwLpY"
      },
      "outputs": [],
      "source": [
        "# packages\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, recall_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ5AMpRzwQhE",
        "outputId": "a130b5a6-2e19-4518-f9ce-1aeea8192e07"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read csv\n",
        "df = pd.read_csv('/content/drive/My Drive/ms_wind_curtailment_prediction/lagged_curtailment_target_features.csv', sep = ';', index_col=0)"
      ],
      "metadata": {
        "id": "BYCqeqUnwTbf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing pipelines\n",
        "preprocessor = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "smote = SMOTE(random_state=13)\n",
        "\n",
        "# create model instance\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100,\n",
        "                                        max_depth=5,\n",
        "                                        min_samples_split=5,\n",
        "                                        min_samples_leaf=1,\n",
        "                                        max_features='sqrt',\n",
        "                                        class_weight='balanced', # assigns higher weights to minority class\n",
        "                                        random_state=42)\n",
        "# features X and target y\n",
        "X = df.drop(['redispatch', 'level'], axis = 1)\n",
        "y = df['redispatch']"
      ],
      "metadata": {
        "id": "wKvcHNLjwUGF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cross-validation\n",
        "n_splits = 70\n",
        "test_size = 24 #(12h with 15 min intervalls)\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits, test_size=test_size)\n",
        "\n",
        "recall_scores = []\n",
        "conf_matrices = []\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    X_train = X_train.copy()\n",
        "    X_test = X_test.copy()\n",
        "\n",
        "    # check if at least one instance of redispatch with status 1 is present in the test data\n",
        "    if y_test.sum() == 0:\n",
        "        continue\n",
        "\n",
        "    # importance of the last redispatch status for the training data\n",
        "    window_size = 2  # last 30 min\n",
        "    last_redispatch_importance_train = []\n",
        "    for i in range(len(y_train)):\n",
        "        window_start = max(0, i - window_size)\n",
        "        window_end = i\n",
        "        importance = y_train.iloc[window_start:window_end].sum() # also incorporate the level\n",
        "        last_redispatch_importance_train.append(importance)\n",
        "    X_train['last_redispatch_importance'] = last_redispatch_importance_train\n",
        "\n",
        "    # rate of decrease for the importance values\n",
        "    decrease_rate = last_redispatch_importance_train[-1] / len(X_test)\n",
        "\n",
        "    # forward fill the last value of 'last_redispatch_importance' for the test data with decreasing values\n",
        "    last_redispatch_importance_test = np.linspace(last_redispatch_importance_train[-1], 0, len(X_test))\n",
        "    X_test['last_redispatch_importance'] = last_redispatch_importance_test\n",
        "\n",
        "    # Preprocess and oversample only the training data\n",
        "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "    X_train_preprocessed, y_train_preprocessed = smote.fit_resample(X_train_preprocessed, y_train)\n",
        "\n",
        "    # Preprocess the test data\n",
        "    X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "    # Fit the model on the preprocessed training data\n",
        "    rf_classifier.fit(X_train_preprocessed, y_train_preprocessed)\n",
        "\n",
        "    # Make predictions on the preprocessed test data\n",
        "    y_pred = rf_classifier.predict(X_test_preprocessed)\n",
        "\n",
        "    # Convert predicted probabilities to binary predictions\n",
        "    y_pred_binary = [1 if pred > 0.3 else 0 for pred in y_pred]\n",
        "\n",
        " #   print(y_test.value_counts())\n",
        "    print(\"printing last redispatch importance \", X_test['last_redispatch_importance'])\n",
        "\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    recall_scores.append(recall)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    conf_matrices.append(conf_matrix)\n",
        "\n",
        "# evaluate\n",
        "print(\"Average Recall Score:\", sum(recall_scores) / len(recall_scores))\n",
        "print(\"Average Confusion Matrix:\", sum(conf_matrices) / len(conf_matrices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISU5-mSewYye",
        "outputId": "07caa5f4-7414-454e-cbd3-84fc5a2a8064"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "printing last redispatch importance  timestamp\n",
            "2023-12-14 06:00:00    0.0\n",
            "2023-12-14 06:15:00    0.0\n",
            "2023-12-14 06:30:00    0.0\n",
            "2023-12-14 06:45:00    0.0\n",
            "2023-12-14 07:00:00    0.0\n",
            "2023-12-14 07:15:00    0.0\n",
            "2023-12-14 07:30:00    0.0\n",
            "2023-12-14 07:45:00    0.0\n",
            "2023-12-14 08:00:00    0.0\n",
            "2023-12-14 08:15:00    0.0\n",
            "2023-12-14 08:30:00    0.0\n",
            "2023-12-14 08:45:00    0.0\n",
            "2023-12-14 09:00:00    0.0\n",
            "2023-12-14 09:15:00    0.0\n",
            "2023-12-14 09:30:00    0.0\n",
            "2023-12-14 09:45:00    0.0\n",
            "2023-12-14 10:00:00    0.0\n",
            "2023-12-14 10:15:00    0.0\n",
            "2023-12-14 10:30:00    0.0\n",
            "2023-12-14 10:45:00    0.0\n",
            "2023-12-14 11:00:00    0.0\n",
            "2023-12-14 11:15:00    0.0\n",
            "2023-12-14 11:30:00    0.0\n",
            "2023-12-14 11:45:00    0.0\n",
            "Name: last_redispatch_importance, dtype: float64\n",
            "printing last redispatch importance  timestamp\n",
            "2023-12-14 12:00:00    2.000000\n",
            "2023-12-14 12:15:00    1.913043\n",
            "2023-12-14 12:30:00    1.826087\n",
            "2023-12-14 12:45:00    1.739130\n",
            "2023-12-14 13:00:00    1.652174\n",
            "2023-12-14 13:15:00    1.565217\n",
            "2023-12-14 13:30:00    1.478261\n",
            "2023-12-14 13:45:00    1.391304\n",
            "2023-12-14 14:00:00    1.304348\n",
            "2023-12-14 14:15:00    1.217391\n",
            "2023-12-14 14:30:00    1.130435\n",
            "2023-12-14 14:45:00    1.043478\n",
            "2023-12-14 15:00:00    0.956522\n",
            "2023-12-14 15:15:00    0.869565\n",
            "2023-12-14 15:30:00    0.782609\n",
            "2023-12-14 15:45:00    0.695652\n",
            "2023-12-14 16:00:00    0.608696\n",
            "2023-12-14 16:15:00    0.521739\n",
            "2023-12-14 16:30:00    0.434783\n",
            "2023-12-14 16:45:00    0.347826\n",
            "2023-12-14 17:00:00    0.260870\n",
            "2023-12-14 17:15:00    0.173913\n",
            "2023-12-14 17:30:00    0.086957\n",
            "2023-12-14 17:45:00    0.000000\n",
            "Name: last_redispatch_importance, dtype: float64\n",
            "Average Recall Score: 0.5\n",
            "Average Confusion Matrix: [[ 3.   1.5]\n",
            " [ 9.5 10. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XilPA3khy0jd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}