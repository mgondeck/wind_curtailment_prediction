{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHjZdg+24I3HdYh1Gfxsiz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Ensemble Method"
      ],
      "metadata": {
        "id": "17sHHJjv1Iq9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rlf5UcMio822"
      },
      "outputs": [],
      "source": [
        "# packages\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, precision_score, f1_score\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScATcGqdqa8C",
        "outputId": "44cb42fb-60cc-43a4-c0fa-4b6920b8fc57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/ms_wind_curtailment_prediction/lagged_curtailment_target_features.csv', sep = ';', index_col=0)"
      ],
      "metadata": {
        "id": "h1SveM0wqcpb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the rows to get appropriate test data\n",
        "start_date = '2022-01-01'\n",
        "end_date = '2022-12-31'\n",
        "df = df.loc[start_date:end_date]"
      ],
      "metadata": {
        "id": "meRHcvTvqevT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# features X and target y\n",
        "X = df.drop(['redispatch', 'level'], axis = 1)\n",
        "y = df['redispatch']"
      ],
      "metadata": {
        "id": "WviUUS0m92Vv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing pipelines\n",
        "preprocessor = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
        "    ('scaler', StandardScaler())#,\n",
        "   # ('feature_selection', SelectKBest(score_func=f_classif, k = 20))\n",
        "])\n",
        "\n",
        "smote = SMOTE(random_state=13)"
      ],
      "metadata": {
        "id": "3g7xRpH0qepR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importance of the last redispatch status for the training data\n",
        "def last_redispatch(y_train, X_test):\n",
        "    \"\"\"\n",
        "    tbd.\n",
        "    \"\"\"\n",
        "    window_size = 2  #last 30 min\n",
        "    last_redispatch_importance_train = []\n",
        "    for i in range(len(y_train)):\n",
        "        window_start = max(0, i - window_size)\n",
        "        window_end = i\n",
        "        importance = y_train.iloc[window_start:window_end].sum() # also incorporate the level ???\n",
        "        last_redispatch_importance_train.append(importance)\n",
        "    # forward fill the last value of 'last_redispatch_importance' for the test data with exponentially decreasing values\n",
        "    decay_factor = 0.9\n",
        "    last_redispatch_importance_test = [last_redispatch_importance_train[-1] * (decay_factor ** i) for i in range(len(X_test))]\n",
        "\n",
        "    return last_redispatch_importance_train, last_redispatch_importance_test"
      ],
      "metadata": {
        "id": "A2d92ZW2quAz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models\n",
        "logistic_reg = LogisticRegression(max_iter=1000, C=0.1)\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "random_forest = RandomForestClassifier(max_depth=1, n_estimators=10, random_state=9)\n",
        "xgb_classifier = XGBClassifier(booster='gbtree', reg_alpha=7, eval_metric='logloss', gamma = 5,\n",
        "                              n_estimators=200, max_depth=6, learning_rate=0.1, objective='binary:logistic', random_state = 13, scale_pos_weight=20)\n",
        "\n",
        "\n",
        "# cross-validation\n",
        "n_splits = 70\n",
        "test_size = 48 #(12h with 15 min intervalls)\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits, test_size=test_size)\n",
        "\n",
        "precision_scores = []\n",
        "f1_scores = []\n",
        "conf_matrices = []\n",
        "precision_train_scores = []\n",
        "f1_train_scores = []\n",
        "conf_train_matrices = []\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    X_train = X_train.copy()\n",
        "    X_test = X_test.copy()\n",
        "\n",
        "    # having at least one instance of redispatch 1 or 0 in test to avoid ill-defined precision/f1 scores\n",
        "    if y_test.sum() == 0 or y_test.sum() == 1:\n",
        "        continue\n",
        "\n",
        "    # add importance of the last redispatch status\n",
        "    last_redispatch_importance_train, last_redispatch_importance_test = last_redispatch(y_train, X_test)\n",
        "    X_train['last_redispatch_importance'] = last_redispatch_importance_train\n",
        "    X_test['last_redispatch_importance'] = last_redispatch_importance_test\n",
        "\n",
        "    # preprocess data\n",
        "    X_train_scaled = preprocessor.fit_transform(X_train, y_train)\n",
        "    X_train_preprocessed, y_train_preprocessed = smote.fit_resample(X_train_scaled, y_train)\n",
        "    X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "    # stacking of heterogenous weak learners and xgboost\n",
        "    estimators_stacking = [\n",
        "        ('lg', logistic_reg),\n",
        "        ('knn', knn_classifier),\n",
        "        ('rf', random_forest)\n",
        "    ]\n",
        "\n",
        "    # Final estimator with XGBoost\n",
        "    final_estimator = xgb_classifier\n",
        "\n",
        "    stacking_classifier = StackingClassifier(estimators=estimators_stacking, final_estimator=final_estimator)\n",
        "\n",
        "    # fit model\n",
        "    stacking_classifier.fit(X_train_preprocessed, y_train_preprocessed)\n",
        "\n",
        "    # Custom threshold\n",
        "    custom_threshold = 0.3\n",
        "\n",
        "    # Make predictions with probability estimates\n",
        "    y_prob = stacking_classifier.predict_proba(X_test_preprocessed)\n",
        "    y_prob_train = stacking_classifier.predict_proba(X_train_scaled)\n",
        "\n",
        "    # Convert probability estimates to binary predictions based on the threshold\n",
        "    y_pred = (y_prob[:, 1] > custom_threshold).astype(int)\n",
        "    y_pred_train = (y_prob_train[:, 1] > custom_threshold).astype(int)\n",
        "\n",
        "    #make predictions\n",
        "    #y_pred = stacking_classifier.predict(X_test_preprocessed)\n",
        "    #y_pred_train = stacking_classifier.predict(X_train_scaled)\n",
        "\n",
        "    #print(y_test.value_counts())\n",
        "    #print(\"last redispatch importance: \", X_test['last_redispatch_importance'])\n",
        "\n",
        "    # evaluate\n",
        "    precision_scores.append(precision_score(y_test, y_pred))\n",
        "    f1_scores.append(f1_score(y_test, y_pred))\n",
        "    conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    precision_train_scores.append(precision_score(y_train, y_pred_train))\n",
        "    f1_train_scores.append(f1_score(y_train, y_pred_train))\n",
        "    conf_train_matrices.append(confusion_matrix(y_train, y_pred_train))\n",
        "\n",
        "\n",
        "# evaluation results\n",
        "print(\"Average Scores:\")\n",
        "print(\"Precision:\", sum(precision_scores) / len(precision_scores))\n",
        "print(\"F1-Scores:\", sum(f1_scores) / len(f1_scores))\n",
        "print(\"Confusion Matrix:\", sum(conf_matrices) / len(conf_matrices))\n",
        "print(\"Precision (Train):\", sum(precision_train_scores) / len(precision_train_scores))\n",
        "print(\"F1-Scores (Train):\", sum(f1_train_scores) / len(f1_train_scores))\n",
        "print(\"Confusion Matrix (Train):\", sum(conf_train_matrices) / len(conf_train_matrices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWV7BWMSqegs",
        "outputId": "e77ea04d-5218-4789-e3cc-79bb7f387485"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Scores:\n",
            "Precision: 0.5856481481481481\n",
            "F1-Scores: 0.23217916261198646\n",
            "Confusion Matrix: [[17.875       0.5       ]\n",
            " [25.33333333  4.29166667]]\n",
            "Precision (Train): 0.9554011778720337\n",
            "F1-Scores (Train): 0.9762928118500356\n",
            "Confusion Matrix (Train): [[2.75178333e+04 2.54000000e+02]\n",
            " [9.00000000e+00 5.38516667e+03]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "xgb_classifier = XGBClassifier(n_estimators=200, scale_pos_weight=20) and random forest\n",
        "Average Scores:\n",
        "Precision: 0.5856481481481481\n",
        "F1-Scores: 0.23217916261198646\n",
        "Confusion Matrix: [[17.875       0.5       ]\n",
        " [25.33333333  4.29166667]]\n",
        "Precision (Train): 0.9554011778720337\n",
        "F1-Scores (Train): 0.9762928118500356\n",
        "Confusion Matrix (Train): [[2.75178333e+04 2.54000000e+02]\n",
        " [9.00000000e+00 5.38516667e+03]]\n",
        "\n",
        "xgb_classifier = XGBClassifier(n_estimators=200, scale_pos_weight=20) AND threshold 0.3\n",
        "Average Scores:\n",
        "Precision: 0.5972222222222222\n",
        "F1-Scores: 0.21781384973677995\n",
        "Confusion Matrix: [[18.     0.375]\n",
        " [26.     3.625]]\n",
        "Precision (Train): 0.9624964642630119\n",
        "F1-Scores (Train): 0.9808142947701771\n",
        "Confusion Matrix (Train): [[2.75613750e+04 2.10458333e+02]\n",
        " [6.66666667e-01 5.39350000e+03]]\n",
        "\n",
        "\n",
        "xgb_classifier = XGBClassifier(n_estimators=200, scale_pos_weight=20)\n",
        "Average Scores:\n",
        "Precision: 0.5972222222222222\n",
        "F1-Scores: 0.21781384973677995\n",
        "Confusion Matrix: [[18.     0.375]\n",
        " [26.     3.625]]\n",
        "Precision (Train): 0.9647228404390352\n",
        "F1-Scores (Train): 0.9815330893810447\n",
        "Confusion Matrix (Train): [[2.75747917e+04 1.97041667e+02]\n",
        " [5.50000000e+00 5.38866667e+03]]\n",
        "\n",
        "xgb_classifier = XGBClassifier(scale_pos_weight=13)\n",
        "Average Scores:\n",
        "Precision: 0.5972222222222222\n",
        "F1-Scores: 0.21641622569764649\n",
        "Confusion Matrix: [[18.          0.375]\n",
        " [26.04166667  3.58333333]]\n",
        "Precision (Train): 0.9669119531330703\n",
        "F1-Scores (Train): 0.9821783677321881\n",
        "Confusion Matrix (Train): [[2.75878333e+04 1.84000000e+02]\n",
        " [1.09583333e+01 5.38320833e+03]]\n",
        "\n",
        "\n",
        "xgb_classifier = XGBClassifier(reg_alpha=7, gamma = 5, max_depth=6, scale_pos_weight=3)\n",
        "Average Scores:\n",
        "Precision: 0.5995370370370371\n",
        "F1-Scores: 0.20899141419020648\n",
        "Confusion Matrix: [[18.16666667  0.20833333]\n",
        " [26.33333333  3.29166667]]\n",
        "Precision (Train): 0.9736545539046618\n",
        "F1-Scores (Train): 0.9717621896797972\n",
        "Confusion Matrix (Train): [[27630.70833333   141.125     ]\n",
        " [  162.125       5232.04166667]]\n",
        "\n",
        "\n",
        "xgb_classifier = XGBClassifier(booster='gbtree', reg_alpha=5, eval_metric='logloss', gamma = 3,\n",
        "                              n_estimators=100, max_depth=6, learning_rate=0.1, objective='binary:logistic')\n",
        "Average Scores:\n",
        "Precision: 0.5995370370370371\n",
        "F1-Scores: 0.20899141419020648\n",
        "Confusion Matrix: [[18.16666667  0.20833333]\n",
        " [26.33333333  3.29166667]]\n",
        "Precision (Train): 0.9736545539046618\n",
        "F1-Scores (Train): 0.9717621896797972\n",
        "Confusion Matrix (Train): [[27630.70833333   141.125     ]\n",
        " [  162.125       5232.04166667]]\n",
        "'''"
      ],
      "metadata": {
        "id": "S26dLgGjcjT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Grid Search"
      ],
      "metadata": {
        "id": "ywsJVNo3bxvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# train/test data\n",
        "cutoff_time = '2023-07-01'  # Define your cutoff time\n",
        "train = df[df.index < cutoff_time]\n",
        "test = df[df.index >= cutoff_time]\n",
        "X_train = train.drop(columns=['redispatch'])\n",
        "y_train = train['redispatch']\n",
        "X_test = test.drop(columns=['redispatch'])\n",
        "y_test = test['redispatch']\n",
        "\n",
        "# preprocess data\n",
        "X_train_scaled = preprocessor.fit_transform(X_train, y_train)\n",
        "X_train_preprocessed, y_train_preprocessed = smote.fit_resample(X_train_scaled, y_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "'''\n",
        "\n",
        "X = df.drop(['redispatch', 'level'], axis = 1)\n",
        "y = df['redispatch']\n",
        "\n",
        "# Define base estimators\n",
        "logistic_reg = LogisticRegression(max_iter=1000, C=0.1)\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "random_forest = RandomForestClassifier(max_depth=1, n_estimators=10, random_state=9)\n",
        "\n",
        "# Define final estimator\n",
        "xgb_classifier = XGBClassifier(booster='gbtree', eval_metric='logloss', objective='binary:logistic',\n",
        "                               random_state=13)\n",
        "\n",
        "# Define the stacking classifier\n",
        "estimators_stacking = [\n",
        "    ('lg', logistic_reg),\n",
        "    ('knn', knn_classifier),\n",
        "    ('rf', logistic_reg)\n",
        "]\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=estimators_stacking, final_estimator=xgb_classifier)\n",
        "\n",
        "stacking_pipeline = Pipeline([\n",
        "    ('stacking', stacking_classifier)\n",
        "])\n",
        "\n",
        "# Define the parameter distributions for random search\n",
        "param_distributions = {\n",
        "    'stacking__final_estimator__n_estimators': randint(50, 100, 150),\n",
        "    'stacking__final_estimator__max_depth': randint(1, 2, 3),\n",
        "    'stacking__final_estimator__learning_rate': [0.1, 0.01, 0.001],\n",
        "    'stacking__final_estimator__gamma': [0, 1, 3],\n",
        "#    'stacking__final_estimator__reg_alpha': [0, 0.2, 0.5],\n",
        "    'stacking__final_estimator__scale_pos_weight': [10, 12, 14]\n",
        "}\n",
        "\n",
        "# Create RandomizedSearchCV instance\n",
        "random_search = RandomizedSearchCV(estimator=stacking_pipeline, param_distributions=param_distributions,\n",
        "                                   n_iter=100, cv=5, scoring='precision', n_jobs=-1, random_state=42)\n",
        "\n",
        "# Fit RandomizedSearchCV to the data\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Score:\", random_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "eiyQ5Eq0twpy",
        "outputId": "40861e41-eb2d-4f0e-d8c3-d8dee8f051c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-2255498b7758>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Fit RandomizedSearchCV to the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Get the best parameters and best score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1706\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# next: GRID SEARCH\n",
        "- Decrease max_depth: Reduce the maximum depth of the trees. Smaller trees are less likely to overfit the training data.\n",
        "- Increase min_child_weight = 3: This parameter specifies the minimum sum of instance weight (hessian) needed in a child. Increasing it makes the algorithm more conservative.\n",
        "- Increase gamma: Gamma specifies the minimum loss reduction required to make a further partition on a leaf node of the tree. Increasing it makes the algorithm more conservative.\n",
        "- Add more training data: If possible, adding more diverse training data can help the model generalize better.\n",
        "\n",
        "\n",
        "### with two lin reg, no feature selection and learning rate 0.001 and target feature thing\n",
        "Average Recall: 1.0\n",
        "Average Accuracy: 0.3072916666666667\n",
        "Average Confusion Matrix: [[ 0.   33.25]\n",
        " [ 0.   14.75]]\n",
        "Average Recall (Train): 1.0\n",
        "Average Accuracy (Train): 0.08297119759437158\n",
        "Average Confusion Matrix (Train): [[    0.   90218.25]\n",
        " [    0.    8162.75]]\n",
        "\n",
        "\n",
        "### with two lin reg, no feature selection and learning rate 0.001\n",
        "Average Recall: 1.0\n",
        "Average Accuracy: 0.3072916666666667\n",
        "Average Confusion Matrix: [[ 0.   33.25]\n",
        " [ 0.   14.75]]\n",
        "Average Recall (Train): 1.0\n",
        "Average Accuracy (Train): 0.08297119759437158\n",
        "Average Confusion Matrix (Train): [[    0.   90218.25]\n",
        " [    0.    8162.75]]\n",
        "\n",
        "### with three lin reg, no feature selection and learning rate 0.001 (no stacking but bagging) (MAYBE AGAIN WITH FEATURE LAGGED TARGET?)\n",
        "To shrink the effect of each tree in gradient boosting and reduce overfitting,\n",
        "you can decrease the learning rate. The learning rate controls the contribution\n",
        "of each tree to the final ensemble model. A lower learning rate means that each\n",
        "tree makes a smaller adjustment to the predictions, which can help prevent\n",
        "overfitting by allowing more trees to be added to the ensemble without\n",
        "over-emphasizing the training data.\n",
        "Average Recall: 1.0\n",
        "Average Accuracy: 0.3072916666666667\n",
        "Average Confusion Matrix: [[ 0.   33.25]\n",
        " [ 0.   14.75]]\n",
        "Average Recall (Train): 1.0\n",
        "Average Accuracy (Train): 0.08297119759437158\n",
        "Average Confusion Matrix (Train): [[    0.   90218.25]\n",
        " [    0.    8162.75]]\n",
        "\n",
        "\n",
        "### with twice lin reg, no feature selection and learning rate 0.01\n",
        "\n",
        "Average Recall: 0.39403953101708517\n",
        "Average Accuracy: 0.6788194444444443\n",
        "Average Confusion Matrix: [[24.5         8.75      ]\n",
        " [ 6.66666667  8.08333333]]\n",
        "\n",
        "Average Recall (Train): 1.0\n",
        "Average Accuracy (Train): 1.0\n",
        "Average Confusion Matrix (Train): [[90197.25     0.  ]\n",
        " [    0.    8162.75]]\n",
        "\n",
        "\n",
        "### with random forest and feature selection and learning rate 0.1\n",
        "Average Recall: 0.29788155028093105\n",
        "Average Accuracy: 0.6041666666666666\n",
        "Average Confusion Matrix: [[25.25  8.  ]\n",
        " [11.    3.75]]\n",
        "Average Recall (Train): 1.0\n",
        "Average Accuracy (Train): 1.0\n",
        "Average Confusion Matrix (Train): [[90197.25     0.  ]\n",
        " [    0.    8162.75]]\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "jiqLCHn5skoC",
        "outputId": "4810556a-1eed-46e3-971e-e85714a9b788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\n### with three lin reg, no feature selection and learning rate 0.001 (no stacking anymore)\\nTo shrink the effect of each tree in gradient boosting and reduce overfitting, \\nyou can decrease the learning rate. The learning rate controls the contribution \\nof each tree to the final ensemble model. A lower learning rate means that each \\ntree makes a smaller adjustment to the predictions, which can help prevent \\noverfitting by allowing more trees to be added to the ensemble without \\nover-emphasizing the training data\\n\\n\\n\\n### with twice lin reg, no feature selection and learning rate 0.01\\nAverage Recall: 0.39403953101708517\\nAverage Accuracy: 0.6788194444444443\\nAverage Confusion Matrix: [[24.5         8.75      ]\\n [ 6.66666667  8.08333333]]\\nAverage Recall (Train): 1.0\\nAverage Accuracy (Train): 1.0\\nAverage Confusion Matrix (Train): [[90197.25     0.  ]\\n [    0.    8162.75]]\\n\\n\\n### with random forest and feature selection and learning rate 0.1\\nAverage Recall: 0.29788155028093105\\nAverage Accuracy: 0.6041666666666666\\nAverage Confusion Matrix: [[25.25  8.  ]\\n [11.    3.75]]\\nAverage Recall (Train): 1.0\\nAverage Accuracy (Train): 1.0\\nAverage Confusion Matrix (Train): [[90197.25     0.  ]\\n [    0.    8162.75]]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eV7NKMovd9o1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}