{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfcRSvtBojIFczll4/ff+B"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Ensemble Method"
      ],
      "metadata": {
        "id": "17sHHJjv1Iq9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rlf5UcMio822"
      },
      "outputs": [],
      "source": [
        "# packages\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "from sklearn.metrics import recall_score, confusion_matrix, accuracy_score, precision_score\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScATcGqdqa8C",
        "outputId": "d3b20a62-8f4b-4c3c-bbbf-94da630b5bba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/ms_wind_curtailment_prediction/lagged_curtailment_target_features.csv', sep = ';', index_col=0)"
      ],
      "metadata": {
        "id": "h1SveM0wqcpb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the rows to get appropriate test data\n",
        "start_date = '2021-01-01'\n",
        "end_date = '2023-11-14'\n",
        "df = df.loc[start_date:end_date]"
      ],
      "metadata": {
        "id": "meRHcvTvqevT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# features X and target y\n",
        "X = df.drop(['redispatch', 'level'], axis = 1)\n",
        "y = df['redispatch']"
      ],
      "metadata": {
        "id": "WviUUS0m92Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing pipelines\n",
        "preprocessor = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
        "    ('scaler', StandardScaler())#,\n",
        "   # ('feature_selection', SelectKBest(score_func=f_classif, k = 20))\n",
        "])\n",
        "\n",
        "smote = SMOTE(random_state=13)"
      ],
      "metadata": {
        "id": "3g7xRpH0qepR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importance of the last redispatch status for the training data\n",
        "def last_redispatch(y_train, X_test):\n",
        "    \"\"\"\n",
        "    tbd.\n",
        "    \"\"\"\n",
        "    window_size = 2  #last 30 min\n",
        "    last_redispatch_importance_train = []\n",
        "    for i in range(len(y_train)):\n",
        "        window_start = max(0, i - window_size)\n",
        "        window_end = i\n",
        "        importance = y_train.iloc[window_start:window_end].sum() # also incorporate the level ???\n",
        "        last_redispatch_importance_train.append(importance)\n",
        "    # forward fill the last value of 'last_redispatch_importance' for the test data with exponentially decreasing values\n",
        "    decay_factor = 0.9\n",
        "    last_redispatch_importance_test = [last_redispatch_importance_train[-1] * (decay_factor ** i) for i in range(len(X_test))]\n",
        "\n",
        "    return last_redispatch_importance_train, last_redispatch_importance_test"
      ],
      "metadata": {
        "id": "A2d92ZW2quAz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models\n",
        "logistic_reg = LogisticRegression(max_iter=1000, C=0.1)  #  regularization parameter C\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "#random_forest = RandomForestClassifier(max_depth=1, n_estimators=10, random_state=9)\n",
        "xgb_classifier = XGBClassifier(booster='gbtree', reg_alpha=6, eval_metric='logloss', gamma=5, min_child_weight = 3,\n",
        "                               n_estimators=100, max_depth=2, learning_rate=0.01, objective='binary:logistic',\n",
        "                               random_state=13, scale_pos_weight=14)\n",
        "\n",
        "\n",
        "# cross-validation\n",
        "n_splits = 70\n",
        "test_size = 48 #(12h with 15 min intervalls)\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits, test_size=test_size)\n",
        "\n",
        "recall_scores = []\n",
        "accuracy_scores = []\n",
        "conf_matrices = []\n",
        "recall_train_scores = []\n",
        "accuracy_train_scores = []\n",
        "conf_train_matrices = []\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    X_train = X_train.copy()\n",
        "    X_test = X_test.copy()\n",
        "\n",
        "    # check if at least one instance of redispatch with status 1 is present in the test data\n",
        "    if y_test.sum() == 0:\n",
        "        continue\n",
        "    '''\n",
        "    # add importance of the last redispatch status\n",
        "    last_redispatch_importance_train, last_redispatch_importance_test = last_redispatch(y_train, X_test)\n",
        "    X_train['last_redispatch_importance'] = last_redispatch_importance_train\n",
        "    X_test['last_redispatch_importance'] = last_redispatch_importance_test\n",
        "    '''\n",
        "    # preprocess data\n",
        "    X_train_scaled = preprocessor.fit_transform(X_train, y_train)\n",
        "    X_train_preprocessed, y_train_preprocessed = smote.fit_resample(X_train_scaled, y_train)\n",
        "    X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "    # stacking of heterogenous weak learners and xgboost\n",
        "    estimators_stacking = [\n",
        "        ('lg', logistic_reg),\n",
        "        ('knn', knn_classifier),\n",
        "        ('rf', logistic_reg)#random_forest\n",
        "    ]\n",
        "\n",
        "    # Final estimator with XGBoost\n",
        "    final_estimator = xgb_classifier\n",
        "\n",
        "    stacking_classifier = StackingClassifier(estimators=estimators_stacking, final_estimator=final_estimator)\n",
        "\n",
        "    # fit model\n",
        "    stacking_classifier.fit(X_train_preprocessed, y_train_preprocessed)\n",
        "\n",
        "    # make predictions\n",
        "    y_pred = stacking_classifier.predict(X_test_preprocessed)\n",
        "    y_pred_train = stacking_classifier.predict(X_train_scaled)\n",
        "\n",
        "    #print(y_test.value_counts())\n",
        "    #print(\"last redispatch importance: \", X_test['last_redispatch_importance'])\n",
        "\n",
        "    # evaluate\n",
        "    recall_scores.append(recall_score(y_test, y_pred))\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "    conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    recall_train_scores.append(recall_score(y_train, y_pred_train))\n",
        "    accuracy_train_scores.append(accuracy_score(y_train, y_pred_train))\n",
        "    conf_train_matrices.append(confusion_matrix(y_train, y_pred_train))\n",
        "\n",
        "    #sorted_idx = stacking_classifier.feature_importances_.argsort()\n",
        "    #plt.barh(X_train.columns[sorted_idx], stacking_classifier.feature_importances_[sorted_idx])\n",
        "\n",
        "\n",
        "# print evaluation results\n",
        "print(\"Average Recall:\", sum(recall_scores) / len(recall_scores))\n",
        "print(\"Average Accuracy:\", sum(accuracy_scores) / len(accuracy_scores))\n",
        "print(\"Average Confusion Matrix:\", sum(conf_matrices) / len(conf_matrices))\n",
        "print(\"Average Recall (Train):\", sum(recall_train_scores) / len(recall_train_scores))\n",
        "print(\"Average Accuracy (Train):\", sum(accuracy_train_scores) / len(accuracy_train_scores))\n",
        "print(\"Average Confusion Matrix (Train):\", sum(conf_train_matrices) / len(conf_train_matrices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWV7BWMSqegs",
        "outputId": "761a73ad-ff4b-4312-e266-ff4b9154aa1d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Recall: 1.0\n",
            "Average Accuracy: 0.3072916666666667\n",
            "Average Confusion Matrix: [[ 0.   33.25]\n",
            " [ 0.   14.75]]\n",
            "Average Recall (Train): 1.0\n",
            "Average Accuracy (Train): 0.08297119759437158\n",
            "Average Confusion Matrix (Train): [[    0.   90218.25]\n",
            " [    0.    8162.75]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# next: GRID SEARCH\n",
        "- Decrease max_depth: Reduce the maximum depth of the trees. Smaller trees are less likely to overfit the training data.\n",
        "- Increase min_child_weight = 3: This parameter specifies the minimum sum of instance weight (hessian) needed in a child. Increasing it makes the algorithm more conservative.\n",
        "- Increase gamma: Gamma specifies the minimum loss reduction required to make a further partition on a leaf node of the tree. Increasing it makes the algorithm more conservative.\n",
        "- Add more training data: If possible, adding more diverse training data can help the model generalize better.\n",
        "\n",
        "\n",
        "### with two lin reg, no feature selection and learning rate 0.001 and target feature thing\n",
        "Average Recall: 1.0\n",
        "Average Accuracy: 0.3072916666666667\n",
        "Average Confusion Matrix: [[ 0.   33.25]\n",
        " [ 0.   14.75]]\n",
        "Average Recall (Train): 1.0\n",
        "Average Accuracy (Train): 0.08297119759437158\n",
        "Average Confusion Matrix (Train): [[    0.   90218.25]\n",
        " [    0.    8162.75]]\n",
        "\n",
        "\n",
        "### with two lin reg, no feature selection and learning rate 0.001\n",
        "Average Recall: 1.0\n",
        "Average Accuracy: 0.3072916666666667\n",
        "Average Confusion Matrix: [[ 0.   33.25]\n",
        " [ 0.   14.75]]\n",
        "Average Recall (Train): 1.0\n",
        "Average Accuracy (Train): 0.08297119759437158\n",
        "Average Confusion Matrix (Train): [[    0.   90218.25]\n",
        " [    0.    8162.75]]\n",
        "\n",
        "### with three lin reg, no feature selection and learning rate 0.001 (no stacking but bagging) (MAYBE AGAIN WITH FEATURE LAGGED TARGET?)\n",
        "To shrink the effect of each tree in gradient boosting and reduce overfitting,\n",
        "you can decrease the learning rate. The learning rate controls the contribution\n",
        "of each tree to the final ensemble model. A lower learning rate means that each\n",
        "tree makes a smaller adjustment to the predictions, which can help prevent\n",
        "overfitting by allowing more trees to be added to the ensemble without\n",
        "over-emphasizing the training data.\n",
        "Average Recall: 1.0\n",
        "Average Accuracy: 0.3072916666666667\n",
        "Average Confusion Matrix: [[ 0.   33.25]\n",
        " [ 0.   14.75]]\n",
        "Average Recall (Train): 1.0\n",
        "Average Accuracy (Train): 0.08297119759437158\n",
        "Average Confusion Matrix (Train): [[    0.   90218.25]\n",
        " [    0.    8162.75]]\n",
        "\n",
        "\n",
        "### with twice lin reg, no feature selection and learning rate 0.01\n",
        "\n",
        "Average Recall: 0.39403953101708517\n",
        "Average Accuracy: 0.6788194444444443\n",
        "Average Confusion Matrix: [[24.5         8.75      ]\n",
        " [ 6.66666667  8.08333333]]\n",
        "\n",
        "Average Recall (Train): 1.0\n",
        "Average Accuracy (Train): 1.0\n",
        "Average Confusion Matrix (Train): [[90197.25     0.  ]\n",
        " [    0.    8162.75]]\n",
        "\n",
        "\n",
        "### with random forest and feature selection and learning rate 0.1\n",
        "Average Recall: 0.29788155028093105\n",
        "Average Accuracy: 0.6041666666666666\n",
        "Average Confusion Matrix: [[25.25  8.  ]\n",
        " [11.    3.75]]\n",
        "Average Recall (Train): 1.0\n",
        "Average Accuracy (Train): 1.0\n",
        "Average Confusion Matrix (Train): [[90197.25     0.  ]\n",
        " [    0.    8162.75]]\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "jiqLCHn5skoC",
        "outputId": "4810556a-1eed-46e3-971e-e85714a9b788"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\n### with three lin reg, no feature selection and learning rate 0.001 (no stacking anymore)\\nTo shrink the effect of each tree in gradient boosting and reduce overfitting, \\nyou can decrease the learning rate. The learning rate controls the contribution \\nof each tree to the final ensemble model. A lower learning rate means that each \\ntree makes a smaller adjustment to the predictions, which can help prevent \\noverfitting by allowing more trees to be added to the ensemble without \\nover-emphasizing the training data\\n\\n\\n\\n### with twice lin reg, no feature selection and learning rate 0.01\\nAverage Recall: 0.39403953101708517\\nAverage Accuracy: 0.6788194444444443\\nAverage Confusion Matrix: [[24.5         8.75      ]\\n [ 6.66666667  8.08333333]]\\nAverage Recall (Train): 1.0\\nAverage Accuracy (Train): 1.0\\nAverage Confusion Matrix (Train): [[90197.25     0.  ]\\n [    0.    8162.75]]\\n\\n\\n### with random forest and feature selection and learning rate 0.1\\nAverage Recall: 0.29788155028093105\\nAverage Accuracy: 0.6041666666666666\\nAverage Confusion Matrix: [[25.25  8.  ]\\n [11.    3.75]]\\nAverage Recall (Train): 1.0\\nAverage Accuracy (Train): 1.0\\nAverage Confusion Matrix (Train): [[90197.25     0.  ]\\n [    0.    8162.75]]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search"
      ],
      "metadata": {
        "id": "ywsJVNo3bxvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# preprocess data\n",
        "X_train_scaled = preprocessor.fit_transform(X_train, y_train)\n",
        "X_train_preprocessed, y_train_preprocessed = smote.fit_resample(X_train_scaled, y_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# models\n",
        "logistic_reg = LogisticRegression(max_iter=1000, C=0.1)  #  regularization parameter C\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "xgb_classifier = XGBClassifier(booster='gbtree', eval_metric='logloss', objective='binary:logistic',\n",
        "                               random_state=13)\n",
        "\n",
        "# create stacking instance\n",
        "estimators_stacking = [\n",
        "    ('lg', logistic_reg),\n",
        "    ('knn', knn_classifier),\n",
        "    ('rf', logistic_reg)#random_forest\n",
        "]\n",
        "final_estimator = xgb_classifier\n",
        "stacking_classifier = StackingClassifier(estimators=estimators_stacking, final_estimator=final_estimator)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [1, 2, 3],\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    'gamma': [0, 2, 5],\n",
        "    'reg_alpha': [0, 0.2, 0.5],\n",
        "    'min_child_weight': [1, 2, 3],\n",
        "    'scale_pos_weight': [10, 12, 14]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV instance\n",
        "grid_search = GridSearchCV(estimator=stacking_classifier, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the data\n",
        "grid_search.fit(X_train_preprocessed, y_train_preprocessed)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "eiyQ5Eq0twpy"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}