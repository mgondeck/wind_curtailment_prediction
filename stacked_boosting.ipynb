{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlxzhLCJ1lnigkLCf4ChNK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Ensemble Method Stacked Boosting"
      ],
      "metadata": {
        "id": "17sHHJjv1Iq9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rlf5UcMio822"
      },
      "outputs": [],
      "source": [
        "# packages\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, precision_score, f1_score\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScATcGqdqa8C",
        "outputId": "44cb42fb-60cc-43a4-c0fa-4b6920b8fc57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/ms_wind_curtailment_prediction/lagged_curtailment_target_features.csv', sep = ';', index_col=0)"
      ],
      "metadata": {
        "id": "h1SveM0wqcpb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the rows to get appropriate test data\n",
        "start_date = '2022-01-01'\n",
        "end_date = '2022-12-31'\n",
        "df = df.loc[start_date:end_date]"
      ],
      "metadata": {
        "id": "meRHcvTvqevT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# features X and target y\n",
        "X = df.drop(['redispatch', 'level'], axis = 1)\n",
        "y = df['redispatch']"
      ],
      "metadata": {
        "id": "WviUUS0m92Vv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing pipelines\n",
        "preprocessor = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
        "    ('scaler', StandardScaler())#,\n",
        "   # ('feature_selection', SelectKBest(score_func=f_classif, k = 20))\n",
        "])\n",
        "\n",
        "smote = SMOTE(random_state=13)"
      ],
      "metadata": {
        "id": "3g7xRpH0qepR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importance of the last redispatch status for the training data\n",
        "def last_redispatch(y_train, X_test):\n",
        "    \"\"\"\n",
        "    tbd.\n",
        "    \"\"\"\n",
        "    window_size = 2  #last 30 min\n",
        "    last_redispatch_importance_train = []\n",
        "    for i in range(len(y_train)):\n",
        "        window_start = max(0, i - window_size)\n",
        "        window_end = i\n",
        "        importance = y_train.iloc[window_start:window_end].sum() # also incorporate the level ???\n",
        "        last_redispatch_importance_train.append(importance)\n",
        "    # forward fill the last value of 'last_redispatch_importance' for the test data with exponentially decreasing values\n",
        "    decay_factor = 0.9\n",
        "    last_redispatch_importance_test = [last_redispatch_importance_train[-1] * (decay_factor ** i) for i in range(len(X_test))]\n",
        "\n",
        "    return last_redispatch_importance_train, last_redispatch_importance_test"
      ],
      "metadata": {
        "id": "A2d92ZW2quAz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models\n",
        "logistic_reg = LogisticRegression(max_iter=1000, C=0.1)\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "random_forest = RandomForestClassifier(max_depth=1, n_estimators=10, random_state=9)\n",
        "xgb_classifier = XGBClassifier(booster='gbtree', reg_alpha=7, eval_metric='logloss', gamma = 5,\n",
        "                              n_estimators=200, max_depth=6, learning_rate=0.1, objective='binary:logistic', random_state = 13)#, scale_pos_weight=20)\n",
        "\n",
        "\n",
        "# cross-validation\n",
        "n_splits = 70\n",
        "test_size = 48 #(12h with 15 min intervalls)\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits, test_size=test_size)\n",
        "\n",
        "precision_scores = []\n",
        "f1_scores = []\n",
        "conf_matrices = []\n",
        "precision_train_scores = []\n",
        "f1_train_scores = []\n",
        "conf_train_matrices = []\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    X_train = X_train.copy()\n",
        "    X_test = X_test.copy()\n",
        "\n",
        "    # having at least one instance of redispatch 1 or 0 in test to avoid ill-defined precision/f1 scores\n",
        "    if y_test.sum() == 0 or y_test.sum() == 1:\n",
        "        continue\n",
        "\n",
        "    # add importance of the last redispatch status\n",
        "    last_redispatch_importance_train, last_redispatch_importance_test = last_redispatch(y_train, X_test)\n",
        "    X_train['last_redispatch_importance'] = last_redispatch_importance_train\n",
        "    X_test['last_redispatch_importance'] = last_redispatch_importance_test\n",
        "\n",
        "    # preprocess data\n",
        "    X_train_scaled = preprocessor.fit_transform(X_train, y_train)\n",
        "    X_train_preprocessed, y_train_preprocessed = smote.fit_resample(X_train_scaled, y_train)\n",
        "    X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "    # stacking of heterogenous weak learners and xgboost\n",
        "    estimators_stacking = [\n",
        "        ('lg', knn_classifier), #logistic_reg\n",
        "        ('knn', knn_classifier), #knn_classifier\n",
        "        ('rf', random_forest)\n",
        "    ]\n",
        "\n",
        "    # Final estimator with XGBoost\n",
        "    final_estimator = xgb_classifier\n",
        "\n",
        "    stacking_classifier = StackingClassifier(estimators=estimators_stacking, final_estimator=final_estimator)\n",
        "\n",
        "    # fit model\n",
        "    stacking_classifier.fit(X_train_preprocessed, y_train_preprocessed)\n",
        "\n",
        "    # Custom threshold\n",
        "    custom_threshold = 0.3\n",
        "\n",
        "    # Make predictions with probability estimates\n",
        "    y_prob = stacking_classifier.predict_proba(X_test_preprocessed)\n",
        "    y_prob_train = stacking_classifier.predict_proba(X_train_scaled)\n",
        "\n",
        "    # Convert probability estimates to binary predictions based on the threshold\n",
        "    y_pred = (y_prob[:, 1] > custom_threshold).astype(int)\n",
        "    y_pred_train = (y_prob_train[:, 1] > custom_threshold).astype(int)\n",
        "\n",
        "    #make predictions\n",
        "    #y_pred = stacking_classifier.predict(X_test_preprocessed)\n",
        "    #y_pred_train = stacking_classifier.predict(X_train_scaled)\n",
        "\n",
        "    #print(y_test.value_counts())\n",
        "    #print(\"last redispatch importance: \", X_test['last_redispatch_importance'])\n",
        "\n",
        "    # evaluate\n",
        "    precision_scores.append(precision_score(y_test, y_pred))\n",
        "    f1_scores.append(f1_score(y_test, y_pred))\n",
        "    conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    precision_train_scores.append(precision_score(y_train, y_pred_train))\n",
        "    f1_train_scores.append(f1_score(y_train, y_pred_train))\n",
        "    conf_train_matrices.append(confusion_matrix(y_train, y_pred_train))\n",
        "\n",
        "\n",
        "# evaluation results\n",
        "print(\"Average Scores:\")\n",
        "print(\"Precision:\", sum(precision_scores) / len(precision_scores))\n",
        "print(\"F1-Scores:\", sum(f1_scores) / len(f1_scores))\n",
        "print(\"Confusion Matrix:\", sum(conf_matrices) / len(conf_matrices))\n",
        "print(\"Precision (Train):\", sum(precision_train_scores) / len(precision_train_scores))\n",
        "print(\"F1-Scores (Train):\", sum(f1_train_scores) / len(f1_train_scores))\n",
        "print(\"Confusion Matrix (Train):\", sum(conf_train_matrices) / len(conf_train_matrices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "ZWV7BWMSqegs",
        "outputId": "0db011cf-2222-4087-8fd3-ac933ee35237"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-81f0d247c1b5>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Make predictions with probability estimates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacking_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_preprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0my_prob_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacking_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Convert probability estimates to binary predictions based on the threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \"\"\"\n\u001b[1;32m    712\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mPrediction\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sk_visual_block_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;34m\"\"\"Concatenate and return the predictions of the estimators.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         predictions = [\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_method_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         predictions = [\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_method_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"drop\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;31m# In that case, we do not need the distances to perform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;31m# the weighting so we do not compute them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mneigh_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    822\u001b[0m         )\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_pairwise_distances_reductions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             results = ArgKmin.compute(\n\u001b[0m\u001b[1;32m    825\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                 \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \"\"\"\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             return ArgKmin64.compute(\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\u001b[0m in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/threadpoolctl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_original_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "## three random forests\n",
        "Average Scores:\n",
        "Precision: 0.6171874999999999\n",
        "F1-Scores: 0.6858921196056901\n",
        "Confusion Matrix: [[20.    38.375]\n",
        " [20.    29.625]]\n",
        "Precision (Train): 0.1626150567610464\n",
        "F1-Scores (Train): 0.2797336672098511\n",
        "Confusion Matrix (Train): [[    0.         27771.83333333]\n",
        " [    0.          5394.16666667]]\n",
        "\n",
        "xgb_classifier = XGBClassifier(n_estimators=200, scale_pos_weight=20) and random forest\n",
        "Average Scores:\n",
        "Precision: 0.5856481481481481\n",
        "F1-Scores: 0.23217916261198646\n",
        "Confusion Matrix: [[17.875       0.5       ]\n",
        " [25.33333333  4.29166667]]\n",
        "Precision (Train): 0.9554011778720337\n",
        "F1-Scores (Train): 0.9762928118500356\n",
        "Confusion Matrix (Train): [[2.75178333e+04 2.54000000e+02]\n",
        " [9.00000000e+00 5.38516667e+03]]\n",
        "\n",
        "xgb_classifier = XGBClassifier(n_estimators=200, scale_pos_weight=20) AND threshold 0.3\n",
        "Average Scores:\n",
        "Precision: 0.5972222222222222\n",
        "F1-Scores: 0.21781384973677995\n",
        "Confusion Matrix: [[18.     0.375]\n",
        " [26.     3.625]]\n",
        "Precision (Train): 0.9624964642630119\n",
        "F1-Scores (Train): 0.9808142947701771\n",
        "Confusion Matrix (Train): [[2.75613750e+04 2.10458333e+02]\n",
        " [6.66666667e-01 5.39350000e+03]]\n",
        "\n",
        "\n",
        "xgb_classifier = XGBClassifier(n_estimators=200, scale_pos_weight=20)\n",
        "Average Scores:\n",
        "Precision: 0.5972222222222222\n",
        "F1-Scores: 0.21781384973677995\n",
        "Confusion Matrix: [[18.     0.375]\n",
        " [26.     3.625]]\n",
        "Precision (Train): 0.9647228404390352\n",
        "F1-Scores (Train): 0.9815330893810447\n",
        "Confusion Matrix (Train): [[2.75747917e+04 1.97041667e+02]\n",
        " [5.50000000e+00 5.38866667e+03]]\n",
        "\n",
        "xgb_classifier = XGBClassifier(scale_pos_weight=13)\n",
        "Average Scores:\n",
        "Precision: 0.5972222222222222\n",
        "F1-Scores: 0.21641622569764649\n",
        "Confusion Matrix: [[18.          0.375]\n",
        " [26.04166667  3.58333333]]\n",
        "Precision (Train): 0.9669119531330703\n",
        "F1-Scores (Train): 0.9821783677321881\n",
        "Confusion Matrix (Train): [[2.75878333e+04 1.84000000e+02]\n",
        " [1.09583333e+01 5.38320833e+03]]\n",
        "\n",
        "\n",
        "xgb_classifier = XGBClassifier(reg_alpha=7, gamma = 5, max_depth=6, scale_pos_weight=3)\n",
        "Average Scores:\n",
        "Precision: 0.5995370370370371\n",
        "F1-Scores: 0.20899141419020648\n",
        "Confusion Matrix: [[18.16666667  0.20833333]\n",
        " [26.33333333  3.29166667]]\n",
        "Precision (Train): 0.9736545539046618\n",
        "F1-Scores (Train): 0.9717621896797972\n",
        "Confusion Matrix (Train): [[27630.70833333   141.125     ]\n",
        " [  162.125       5232.04166667]]\n",
        "\n",
        "\n",
        "xgb_classifier = XGBClassifier(booster='gbtree', reg_alpha=5, eval_metric='logloss', gamma = 3,\n",
        "                              n_estimators=100, max_depth=6, learning_rate=0.1, objective='binary:logistic')\n",
        "Average Scores:\n",
        "Precision: 0.5995370370370371\n",
        "F1-Scores: 0.20899141419020648\n",
        "Confusion Matrix: [[18.16666667  0.20833333]\n",
        " [26.33333333  3.29166667]]\n",
        "Precision (Train): 0.9736545539046618\n",
        "F1-Scores (Train): 0.9717621896797972\n",
        "Confusion Matrix (Train): [[27630.70833333   141.125     ]\n",
        " [  162.125       5232.04166667]]\n",
        "'''"
      ],
      "metadata": {
        "id": "S26dLgGjcjT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eV7NKMovd9o1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}