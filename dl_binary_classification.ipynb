{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgBSHlQBgOtkck0xzX1d3I"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ijerM10CgCXa"
      },
      "outputs": [],
      "source": [
        "# packages\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmFcTXOro9wR",
        "outputId": "f256fb3a-eb74-4fb3-a915-70a039eae292"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/ms_wind_curtailment_prediction/curtailment_target_features.csv', sep = ';', index_col=0)"
      ],
      "metadata": {
        "id": "nMsDoQL7pF5r"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the entire dataset into training and test sets\n",
        "cutoff_time = \"2023-01-01\"\n",
        "train = df[df.index < cutoff_time]\n",
        "test = df[df.index >= cutoff_time]\n",
        "X_train = train.drop('redispatch', axis = 1)\n",
        "y_train = train['redispatch']\n",
        "X_test = test.drop('redispatch', axis = 1)\n",
        "y_test = test['redispatch']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m5AASnmpYvo",
        "outputId": "6335d4b3-5968-47a8-b4a7-a09b6c1928d7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "redispatch                           0\n",
              "wind_speed_m/s                      16\n",
              "wind_direction_degrees              16\n",
              "radiation_global_J/m2               16\n",
              "air_temperature_K                   16\n",
              "humidity_percent                    16\n",
              "wind_gust_max_m/s                   16\n",
              "wind_direction_gust_max_degrees     16\n",
              "forecast_solar_MW                  112\n",
              "actual_solar_MW                    197\n",
              "total_grid_load_MWh                 16\n",
              "residual_load_MWh                   16\n",
              "pumped_storage_MWh                  16\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the preprocessing steps\n",
        "imputer = SimpleImputer(strategy='constant', fill_value=0)  # Replace missing values with 0\n",
        "scaler = StandardScaler()\n",
        "smote = SMOTE()\n",
        "# Create the preprocessing pipeline\n",
        "preprocessor = Pipeline([\n",
        "    ('imputer', imputer),\n",
        "    ('scaler', scaler),\n",
        "    ('smote', smote)\n",
        "])\n",
        "\n",
        "# Example usage:\n",
        "# Fit the pipeline on training data and transform it\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train, y_train)\n",
        "\n",
        "# Transform the test data using the fitted pipeline\n",
        "X_test_preprocessed = preprocessor.transform(X_test)"
      ],
      "metadata": {
        "id": "nojG2tG1pu7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dictionary to store results\n",
        "training_history = {}\n",
        "\n",
        "# Define number of epochs and learning rate decay\n",
        "N_TRAIN = len(X_train)\n",
        "EPOCHS = 2000\n",
        "BATCH_SIZE = 32\n",
        "STEPS_PER_EPOCH = N_TRAIN // BATCH_SIZE\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    0.01,\n",
        "    decay_steps=STEPS_PER_EPOCH*1000,\n",
        "    decay_rate=1,\n",
        "    staircase=False)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, name='Adam')\n",
        "\n",
        "# define model\n",
        "def get_compiled_large_model():\n",
        "    large_model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(512,kernel_initializer = 'uniform', activation='relu', input_dim = 12),\n",
        "      tf.keras.layers.Dense(512,kernel_initializer = 'uniform', activation='relu'),\n",
        "      tf.keras.layers.Dense(512,kernel_initializer = 'uniform', activation='relu'),\n",
        "      tf.keras.layers.Dense(512,kernel_initializer = 'uniform', activation='relu'),\n",
        "      tf.keras.layers.Dense(1,kernel_initializer = 'uniform')\n",
        "    ])\n",
        "\n",
        "    large_model.compile(optimizer=optimizer,\n",
        "                  loss='mae',\n",
        "                  metrics=['mse'])\n",
        "    return large_model\n",
        "\n",
        "# Summary of large sized model\n",
        "with tf.device('/cpu:0'):\n",
        "    large_model = get_compiled_large_model()\n",
        "    print(large_model.summary())\n",
        "\n",
        "# train\n",
        "# Train the model with the new callback\n",
        "with tf.device('/cpu:0'):\n",
        "    training_history['large'] = large_model.fit(X_train,\n",
        "                        y_train,\n",
        "                        validation_split=0.2,\n",
        "                        verbose=0,\n",
        "                        steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                        epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "FAlojD6KrHPs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}